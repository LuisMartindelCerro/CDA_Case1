{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('../Data/data_5KNN.csv')\n",
    "\n",
    "# Pick the most important variables\n",
    "\n",
    "top12_variables = [\"y\", \"x_32\", \"x_62\",\"x_36\", \"x_57\",\"x_10\", \"x_30\", \"x_41\",\"x_45\", \"x_29\",\"x_54\", \"x_76\", \"x_64\"]\n",
    "df = df[top12_variables]\n",
    "\n",
    "data = df.to_numpy()\n",
    "\n",
    "# Get the variables\n",
    "X = data[:,1:]\n",
    "y = data[:,0]\n",
    "\n",
    "# Shape\n",
    "[n, p] = np.shape(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2-Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoLars, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer RMSE - OLS: 25.3412, Ridge: 25.1617, Lasso: 25.2994, ElasticNet: 25.1626\n",
      "Outer RMSE - OLS: 18.5630, Ridge: 17.7765, Lasso: 18.3776, ElasticNet: 17.7125\n",
      "Outer RMSE - OLS: 30.9258, Ridge: 30.7199, Lasso: 30.5194, ElasticNet: 30.4364\n",
      "Outer RMSE - OLS: 30.1668, Ridge: 30.3201, Lasso: 30.6516, ElasticNet: 30.6275\n",
      "Outer RMSE - OLS: 23.1708, Ridge: 22.9999, Lasso: 23.0083, ElasticNet: 22.9457\n",
      "\n",
      "Average RMSE across all folds:\n",
      "OLS: 25.6335\n",
      "Ridge: 25.3956\n",
      "Lasso: 25.5712\n",
      "ElasticNet: 25.3769\n",
      "\n",
      "Best model overall: ElasticNet with RMSE = 25.3769\n",
      "\n",
      "Ridge alpha parameters across folds: ['1.92', '2.42', '0.85', '1.20', '2.42']\n",
      "Lasso alpha parameters across folds: ['0.38', '0.21', '0.53', '0.48', '0.53']\n",
      "ElasticNet parameters across folds (alpha, l1_ratio): ['(0.13, 0.81)', '(0.04, 0.15)', '(0.24, 0.95)', '(0.30, 0.95)', '(0.48, 0.95)']\n",
      "\n",
      "Creating final ElasticNet model with averaged alpha = 0.2367 and l1_ratio = 0.7611\n",
      "Final model in-sample RMSE: 22.9270\n"
     ]
    }
   ],
   "source": [
    "# 1. Outer K-Fold\n",
    "K_outer = 5\n",
    "outer_cv = KFold(n_splits=K_outer)\n",
    "\n",
    "# Define a range of alphas to try\n",
    "lambda_values = np.logspace(-3, 2, 100)\n",
    "l1_ratios = np.linspace(0.1, 0.95, 19)   # Mix ratio between Lasso (L1) and Ridge (L2)\n",
    "\n",
    "# Store the outer test RMSE for each model type\n",
    "ols_outer_rmse = []\n",
    "ridge_outer_rmse = []\n",
    "lasso_outer_rmse = []\n",
    "elasticnet_outer_rmse = []\n",
    "\n",
    "# Store the best models from each fold\n",
    "best_ols_models = []\n",
    "best_ridge_models = []\n",
    "best_lasso_models = []\n",
    "best_elasticnet_models = []\n",
    "\n",
    "# Store the best parameters from each fold\n",
    "best_ridge_params = []\n",
    "best_lasso_params = []\n",
    "best_elasticnet_params = []\n",
    "\n",
    "# 2. Inner CV Loop\n",
    "for train_idx, test_idx in outer_cv.split(X):\n",
    "    # Split data into inner training and outer test sets\n",
    "    X_train_outer, X_test_outer = X[train_idx], X[test_idx]\n",
    "    y_train_outer, y_test_outer = y[train_idx], y[test_idx]\n",
    "\n",
    "    # 2.0 Inner CV: OLS (Linear Regression)\n",
    "    # Define a pipeline that includes normalization and OLS regression\n",
    "    ols_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Standardize features\n",
    "        ('ols', LinearRegression())    # Apply OLS Regression\n",
    "    ])\n",
    "    \n",
    "    # Fit OLS model on the outer training set\n",
    "    ols_pipeline.fit(X_train_outer, y_train_outer)\n",
    "    \n",
    "    # Calculate OLS RMSE using cross-validation on the outer training set\n",
    "    ols_cv = GridSearchCV(\n",
    "        estimator=ols_pipeline,\n",
    "        param_grid={},  # No hyperparameters to tune for OLS\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=KFold(n_splits=5)\n",
    "    )\n",
    "    ols_cv.fit(X_train_outer, y_train_outer)\n",
    "    \n",
    "    # Obtain the model, score (MSE) and RMSE\n",
    "    best_ols_model = ols_pipeline  # The fitted pipeline\n",
    "    best_ols_models.append(best_ols_model)\n",
    "    best_ols_score = ols_cv.best_score_  # negative MSE\n",
    "    best_ols_rmse = np.sqrt(-best_ols_score)  # RMSE\n",
    "\n",
    "    # 2.1 Inner CV: RIDGE\n",
    "    # Define a pipeline that includes normalization and regression to indtroduce in the GridSearchCV\n",
    "    ridge_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Standardize features\n",
    "        ('ridge', Ridge())             # Apply Ridge Regression\n",
    "    ])\n",
    "    \n",
    "    # We need a dictionary as an input for the parameters\n",
    "    ridge_param_grid = {'ridge__alpha' : lambda_values}\n",
    "\n",
    "    # Perform GridSearchCV to find the best lambda (alpha in this case)\n",
    "    ridge_cv = GridSearchCV(\n",
    "        estimator=ridge_pipeline,\n",
    "        param_grid=ridge_param_grid,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=KFold(n_splits=5)\n",
    "    )\n",
    "    ridge_cv.fit(X_train_outer, y_train_outer)  # it is using the normalized data stablished in the pipeline\n",
    "    \n",
    "    # Obtain the best parameters and the corresponding model\n",
    "    best_lambda_ridge = ridge_cv.best_params_['ridge__alpha']\n",
    "    best_ridge_params.append(best_lambda_ridge)\n",
    "\n",
    "    # Obtain the best model, score (MSE) and RMSE\n",
    "    best_ridge_model = ridge_cv.best_estimator_  # pipeline refit on entire outer-training set\n",
    "    best_ridge_models.append(best_ridge_model)\n",
    "    best_ridge_score = ridge_cv.best_score_      # negative MSE (the higher, the better)\n",
    "    best_ridge_rmse = np.sqrt(-best_ridge_score)  # turn negative MSE into positive MSE and obtain RMSE\n",
    "\n",
    "    # 2.2 Inner CV: LASSO LARS\n",
    "    # Define a pipeline that includes normalization and regression to indtroduce in the GridSearchCV\n",
    "    lasso_lars_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Standardize features\n",
    "        ('lasso_lars', LassoLars())             # Apply Lasso LARS Regression\n",
    "    ])\n",
    "    \n",
    "    # We need a dictionary as an input for the parameters\n",
    "    lasso_lars_param_grid = {'lasso_lars__alpha' : lambda_values}\n",
    "\n",
    "    # Perform GridSearchCV to find the best lambda (alpha in this case)\n",
    "    lasso_lars_cv = GridSearchCV(\n",
    "        estimator=lasso_lars_pipeline,\n",
    "        param_grid=lasso_lars_param_grid,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=KFold(n_splits=5)\n",
    "    )\n",
    "\n",
    "    lasso_lars_cv.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # Obtain the best parameters and the corresponding model\n",
    "    best_lambda_lasso_lars = lasso_lars_cv.best_params_['lasso_lars__alpha']\n",
    "    best_lasso_params.append(best_lambda_lasso_lars)\n",
    "\n",
    "    # Obtain the best model, score (MSE) and RMSE\n",
    "    best_lasso_model = lasso_lars_cv.best_estimator_\n",
    "    best_lasso_models.append(best_lasso_model)\n",
    "    best_lasso_score = lasso_lars_cv.best_score_\n",
    "    best_lasso_rmse = np.sqrt(-best_lasso_score)\n",
    "\n",
    "    # 2.3 Inner CV: ELASTICNET\n",
    "    # Define a pipeline that includes normalization and regression to introduce in GridSearchCV\n",
    "    elasticnet_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Standardize features\n",
    "        ('elasticnet', ElasticNet(max_iter=20000))  # Apply ElasticNet with a high iteration limit\n",
    "    ])\n",
    "\n",
    "    # We need a dictionary as an input for the parameters\n",
    "    elasticnet_param_grid = {\n",
    "        'elasticnet__alpha': lambda_values,\n",
    "        'elasticnet__l1_ratio': l1_ratios  # The mix ratio between Lasso and Ridge\n",
    "    }\n",
    "\n",
    "    # Perform GridSearchCV to find the best lambda (alpha in this case)\n",
    "    elasticnet_cv = GridSearchCV(\n",
    "        estimator=elasticnet_pipeline,\n",
    "        param_grid=elasticnet_param_grid,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=KFold(n_splits=5),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    elasticnet_cv.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # Obtain the best parameters and the corresponding model\n",
    "    best_lambda_elasticnet = elasticnet_cv.best_params_['elasticnet__alpha']\n",
    "    best_l1_ratio_elasticnet = elasticnet_cv.best_params_['elasticnet__l1_ratio']\n",
    "    best_elasticnet_params.append((best_lambda_elasticnet, best_l1_ratio_elasticnet))\n",
    "\n",
    "    # Obtain the best model, score (MSE) and RMSE\n",
    "    best_elasticnet_model = elasticnet_cv.best_estimator_\n",
    "    best_elasticnet_models.append(best_elasticnet_model)\n",
    "    best_elasticnet_score = elasticnet_cv.best_score_\n",
    "    best_elasticnet_rmse = np.sqrt(-best_elasticnet_score)\n",
    "\n",
    "    # 2.4 Evaluate all models on the Outer Test Fold\n",
    "    # OLS prediction\n",
    "    y_pred_ols = best_ols_model.predict(X_test_outer)\n",
    "    ols_rmse = root_mean_squared_error(y_test_outer, y_pred_ols)\n",
    "    ols_outer_rmse.append(ols_rmse)\n",
    "    \n",
    "    # Ridge prediction\n",
    "    y_pred_ridge = best_ridge_model.predict(X_test_outer)\n",
    "    ridge_rmse = root_mean_squared_error(y_test_outer, y_pred_ridge)\n",
    "    ridge_outer_rmse.append(ridge_rmse)\n",
    "    \n",
    "    # Lasso prediction\n",
    "    y_pred_lasso = best_lasso_model.predict(X_test_outer)\n",
    "    lasso_rmse = root_mean_squared_error(y_test_outer, y_pred_lasso)\n",
    "    lasso_outer_rmse.append(lasso_rmse)\n",
    "    \n",
    "    # ElasticNet prediction\n",
    "    y_pred_elasticnet = best_elasticnet_model.predict(X_test_outer)\n",
    "    elasticnet_rmse = root_mean_squared_error(y_test_outer, y_pred_elasticnet)\n",
    "    elasticnet_outer_rmse.append(elasticnet_rmse)\n",
    "    \n",
    "    # Print results for this fold\n",
    "    print(f\"Outer RMSE - OLS: {ols_rmse:.4f}, Ridge: {ridge_rmse:.4f}, Lasso: {lasso_rmse:.4f}, ElasticNet: {elasticnet_rmse:.4f}\")\n",
    "\n",
    "# 3. Final Estimate of Generalization Error (in terms of RMSE) for each model\n",
    "ols_cv_rmse = np.mean(ols_outer_rmse)\n",
    "ridge_cv_rmse = np.mean(ridge_outer_rmse)\n",
    "lasso_cv_rmse = np.mean(lasso_outer_rmse)\n",
    "elasticnet_cv_rmse = np.mean(elasticnet_outer_rmse)\n",
    "\n",
    "print(\"\\nAverage RMSE across all folds:\")\n",
    "print(f\"OLS: {ols_cv_rmse:.4f}\")\n",
    "print(f\"Ridge: {ridge_cv_rmse:.4f}\")\n",
    "print(f\"Lasso: {lasso_cv_rmse:.4f}\")\n",
    "print(f\"ElasticNet: {elasticnet_cv_rmse:.4f}\")\n",
    "\n",
    "# Determine the best overall model\n",
    "model_comparison = {\n",
    "    'OLS': ols_cv_rmse,\n",
    "    'Ridge': ridge_cv_rmse,\n",
    "    'Lasso': lasso_cv_rmse,\n",
    "    'ElasticNet': elasticnet_cv_rmse\n",
    "}\n",
    "best_model_overall = min(model_comparison, key=model_comparison.get)\n",
    "print(f\"\\nBest model overall: {best_model_overall} with RMSE = {model_comparison[best_model_overall]:.4f}\")\n",
    "\n",
    "# Display the parameters for each model across folds\n",
    "print(\"\\nRidge alpha parameters across folds:\", [f\"{x:.2f}\" for x in best_ridge_params])\n",
    "print(\"Lasso alpha parameters across folds:\", [f\"{x:.2f}\" for x in best_lasso_params])\n",
    "print(\"ElasticNet parameters across folds (alpha, l1_ratio):\", [f\"({a:.2f}, {lr:.2f})\" for a, lr in best_elasticnet_params])\n",
    "\n",
    "# Create a new model with averaged parameters from the best model type\n",
    "if best_model_overall == 'OLS':\n",
    "    final_model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('ols', LinearRegression())\n",
    "    ])\n",
    "    print(f\"\\nCreating final OLS model\")\n",
    "    \n",
    "elif best_model_overall == 'Ridge':\n",
    "    avg_alpha = np.mean(best_ridge_params)\n",
    "    final_model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('ridge', Ridge(alpha=avg_alpha))\n",
    "    ])\n",
    "    print(f\"\\nCreating final Ridge model with averaged alpha = {avg_alpha:.4f}\")\n",
    "    \n",
    "elif best_model_overall == 'Lasso':\n",
    "    avg_alpha = np.mean(best_lasso_params)\n",
    "    final_model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('lasso_lars', LassoLars(alpha=avg_alpha))\n",
    "    ])\n",
    "    print(f\"\\nCreating final Lasso model with averaged alpha = {avg_alpha:.4f}\")\n",
    "    \n",
    "elif best_model_overall == 'ElasticNet':\n",
    "    avg_alpha = np.mean([params[0] for params in best_elasticnet_params])\n",
    "    avg_l1_ratio = np.mean([params[1] for params in best_elasticnet_params])\n",
    "    final_model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('elasticnet', ElasticNet(alpha=avg_alpha, l1_ratio=avg_l1_ratio, max_iter=20000))\n",
    "    ])\n",
    "    print(f\"\\nCreating final ElasticNet model with averaged alpha = {avg_alpha:.4f} and l1_ratio = {avg_l1_ratio:.4f}\")\n",
    "\n",
    "# Fit the final model on the entire dataset\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Evaluate the final model on the entire dataset (in-sample error)\n",
    "y_pred = final_model.predict(X)\n",
    "final_rmse = root_mean_squared_error(y, y_pred)\n",
    "print(f\"Final model in-sample RMSE: {final_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = final_model.named_steps['elasticnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 non-zero coefficients out of 12.\n"
     ]
    }
   ],
   "source": [
    "best_model = ElasticNet(alpha=avg_alpha, l1_ratio=avg_l1_ratio, max_iter=20000)\n",
    "best_model.fit(StandardScaler().fit_transform(X), y)  # Transform X before fitting\n",
    "# Lasso model coefficients are:\n",
    "elasticnet_coef = best_model.coef_\n",
    "print(f'There are {sum(elasticnet_coef != 0)} non-zero coefficients out of {len(elasticnet_coef)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = LassoLars(alpha=avg_alpha)\n",
    "best_model.fit(scaler.fit_transform(X), y)  # Transform X before fitting\n",
    "# Lasso model coefficients are:\n",
    "lasso_coef = best_model.coef_\n",
    "print(f'There are {sum(lasso_coef != 0)} non-zero coefficients out of {len(lasso_coef)}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
